{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning (10 Questions, 10 Marks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color: blue;text-decoration:underline;font-weight: bold;text-align:center;\">Question 54</span>\n",
    "What can be a consequence of using too few principal components in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using too few principal components in Principal Component Analysis (PCA) can lead to <span style = \"color: green;text-decoration:underline;\">**underfitting due to loss of important information**</span>. When the number of principal components is limited, the model may not capture the underlying structure of the data adequately, resulting in a failure to represent significant patterns or variations present in the original dataset. This loss of information can degrade the model's performance, as it may overlook critical features that contribute to the data's variability and relationships[1][2][5].\n",
    "\n",
    "In contrast, using too many principal components can lead to overfitting, where the model becomes overly complex and sensitive to noise in the training data. However, the primary consequence of using too few components is the risk of underfitting, as the model may not generalize well to new, unseen data due to its inability to encapsulate the essential characteristics of the dataset[2][4].\n",
    "\n",
    "Citations:   \n",
    "[1] https://stats.stackexchange.com/questions/257834/how-does-pca-represent-all-data-with-just-a-few-principal-components  \n",
    "[2] https://towardsdatascience.com/too-many-features-lets-look-at-principal-component-analysis-62504b791ae9?gi=9d56750106ec  \n",
    "[3] https://stats.stackexchange.com/questions/52773/what-can-cause-pca-to-worsen-results-of-a-classifier  \n",
    "[4] https://onlinelibrary.wiley.com/doi/full/10.1111/evo.13835   \n",
    "[5] https://builtin.com/data-science/step-step-explanation-principal-component-analysis   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color: yellow;text-decoration:underline;font-weight: bold;text-align:center;\">Question 55</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-style: italic\">Which of the following highlights the main objective of dimensionality reduction techniques? </span>\n",
    "\n",
    "- To classify data into distinct categories. \n",
    "- To maximize the correlation between features.  \n",
    "- <span style = \"color: yellow\">To reduce the number of features while minimizing data loss. </span>   \n",
    "- To increase the computational complexity of models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of dimensionality reduction techniques is **to reduce the number of features while minimizing data loss**. \n",
    "\n",
    "### Explanation\n",
    "Dimensionality reduction aims to simplify datasets by decreasing the number of input variables (features) while preserving as much relevant information as possible. This process helps in several ways:\n",
    "\n",
    "- **Improved Model Performance**: By reducing the dimensionality, models can become more efficient and less prone to overfitting.\n",
    "- **Enhanced Visualization**: Lower-dimensional representations make it easier to visualize complex data.\n",
    "- **Reduced Computational Cost**: Fewer features lead to faster training times and reduced resource requirements.\n",
    "\n",
    "Overall, the goal is to maintain the integrity of the data while simplifying it for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color: pink;font-weight:bold; text-decoration:underline;\">Question 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-style: italic;\">What happens to the explained variance as more principal components are added?</span>\n",
    "\n",
    "- <span style = \"color: pink; text-decoration: underline;\">It initially increases and then plateaus. </span>\n",
    "- It continues to increase indefinitely. \n",
    "- It decreases. \n",
    "- It remains constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more principal components are added in Principal Component Analysis (PCA), the **explained variance initially increases and then plateaus**. \n",
    "\n",
    "### Explanation\n",
    "1. **Initial Increase**: The first principal component captures the maximum variance possible from the dataset. Each subsequent component captures the next highest variance, but this amount typically decreases.\n",
    "\n",
    "2. **Plateauing Effect**: After a certain number of components, the additional components contribute less and less to the explained variance. Eventually, the increase in explained variance becomes negligible, leading to a plateau effect.\n",
    "\n",
    "This behavior reflects the nature of PCA, where the components are ordered by the amount of variance they explain, resulting in diminishing returns as more components are included[1][2][3].\n",
    "\n",
    "Citations:\n",
    "[1] https://ro-che.info/articles/2017-12-11-pca-explained-variance\n",
    "[2] https://vitalflux.com/pca-explained-variance-concept-python-example/\n",
    "[3] https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained\n",
    "[4] https://stats.oarc.ucla.edu/spss/seminars/efa-spss/\n",
    "[5] https://builtin.com/data-science/step-step-explanation-principal-component-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style = \"color: gold; font-weight:bold;text-decoration:underline\">Question 57</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which dimensionality reduction technique is known for preserving both local and global structures in the data?*\n",
    "\n",
    "- Linear Discriminant Analysis\n",
    "- Principal Component Analysis\n",
    "- Multi-Dimensional Scaling\n",
    "- <span style = \"color: gold;\">t-SNE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality reduction technique known for preserving both local and global structures in the data is **t-SNE** (t-Distributed Stochastic Neighbor Embedding). \n",
    "\n",
    "### Explanation\n",
    "t-SNE is particularly effective in maintaining the relationships between data points, ensuring that similar points remain close to each other in the reduced-dimensional space while also preserving the overall structure of the data. This makes it suitable for visualizing high-dimensional data in a way that reflects both local and global patterns.\n",
    "\n",
    "While other techniques like PCA focus on maximizing variance and may lose local structure, t-SNE is designed to capture the complex relationships within the data, making it a popular choice for tasks such as clustering and visualization in machine learning and data analysis contexts.\n",
    "\n",
    "Citations:  \n",
    "[1] https://www.matec-conferences.org/articles/matecconf/pdf/2015/09/matecconf_icame2015_06003.pdf   \n",
    "[2] https://www.sciencedirect.com/science/article/am/pii/S0950705116304798  \n",
    "[3] https://www.nature.com/articles/s42003-022-03628-x   \n",
    "[4] https://www.sciencedirect.com/science/article/abs/pii/S0031320323003643   \n",
    "[5] https://datascience.stackexchange.com/questions/109276/what-is-the-meaning-of-preserving-local-or-global-structure-of-the-data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color :violet;text-decoration:underline;font-weight:bold\">Question 58</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is the purpose of scaling the data before applying K-means clustering?*\n",
    "\n",
    "- To reduce the number of clusters in the dataset.\n",
    "- To eliminate outliers from the dataset.\n",
    "- <span style = \"color: violet\">To ensure that all features contribute equally to the distance calculations. </span>\n",
    "- To increase the computational efficiency of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of scaling the data before applying K-means clustering is **to ensure that all features contribute equally to the distance calculations**. \n",
    "\n",
    "K-means clustering relies on distance metrics (typically Euclidean distance) to assign data points to clusters based on their proximity to cluster centroids. If the features in the dataset are on different scales, those with larger ranges can disproportionately influence the distance calculations, leading to biased clustering results. For example, if one feature ranges from 0 to 1 and another from 0 to 1000, the second feature will dominate the distance computations, potentially overshadowing the contributions of the first feature.\n",
    "\n",
    "By scaling the data, such as through standardization (z-score normalization) or min-max scaling, all features are brought to a common scale. This allows each feature to contribute equally to the distance calculations, improving the clustering performance and ensuring that the resulting clusters are more meaningful and representative of the underlying data structure[1][2][3][6]. \n",
    "\n",
    "In summary, scaling is a critical preprocessing step in K-means clustering to avoid bias and achieve better clustering outcomes.\n",
    "\n",
    "Citations:   \n",
    "[1] https://stats.stackexchange.com/questions/89809/is-it-important-to-scale-data-before-clustering  \n",
    "[2] https://stackoverflow.com/questions/57507584/feature-scaling-for-kmeans-algorithm   \n",
    "[3] https://towardsdatascience.com/a-practical-guide-on-k-means-clustering-ca3bef3c853d   \n",
    "[4] https://engineering.teknasyon.com/how-to-normalize-your-unsupervised-data-for-clustering-methods-9389298d20d5  \n",
    "[5] https://www.researchgate.net/post/K-Means_clustering_Is_feature_scaling_a_necessary_pre-processing_step  \n",
    "[6] https://www.geeksforgeeks.org/do-clustering-algorithms-need-feature-scaling-in-the-pre-processing-stage/   \n",
    "[7] https://scikit-learn.org/dev/auto_examples/preprocessing/plot_scaling_importance.html   \n",
    "[8] https://howtolearnmachinelearning.com/articles/feature_scaling_machine_learning/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red;text-decoration:underline;font-weight:bold \">Question 59 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the assumption made about the shape of each cluster in a Gaussian mixture model (GMM)? Each cluster isâ€¦\n",
    "\n",
    "- Rectangular\n",
    "- Linear\n",
    "- <span style = \"color: red\">Ellipsoidal </span>\n",
    "- Circular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Gaussian mixture model (GMM), each cluster is assumed to be **ellipsoidal**. This means that the clusters can take on elliptical shapes, which allows GMMs to model data with varying shapes and sizes effectively. Each Gaussian component in the mixture is characterized by its mean (the center of the cluster) and a covariance matrix, which defines the shape and orientation of the ellipsoid. This flexibility makes GMMs suitable for capturing complex data distributions that may not conform to simple geometric shapes like circles or rectangles[1][2][4].\n",
    "\n",
    "Citations:  \n",
    "[1] https://www.geeksforgeeks.org/gaussian-mixture-model/  \n",
    "[2] https://www.geeksforgeeks.org/what-is-gaussian-mixture-model-clustering-using-r/  \n",
    "[3]https://www.kaggle.com/code/vipulgandhi/gaussian-mixture-models-clustering-explained    \n",
    "[4] https://lamarr-institute.org/blog/clustering-gaussian-mixture-models/    \n",
    "[5] https://stats.stackexchange.com/questions/543748/assumption-of-gaussian-mixture-model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color:lime;text-decoration:underline;font-weight:bold \">Question 60 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are subjective measures of recommender system performance different from objective measures?\n",
    "\n",
    "- <span style = \"color: lime\">Subjective measures are based on user interpretation, while objective measures rely on algorithmic performance.\n",
    "- Subjective measures provide results independent of user interpretation, while objective measures require user judgement.\n",
    "- Subjective measures are quantitative and objective measures are qualitative.\n",
    "- Subjective measures evaluate actual preferences, while objective measures assess recommendation list coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjective and objective measures of recommender system performance differ fundamentally in their approach to evaluation.\n",
    "\n",
    "## Subjective Measures\n",
    "\n",
    "Subjective measures are based on user perceptions and experiences. They typically involve collecting qualitative feedback from users through surveys, questionnaires, or interviews after they interact with the recommender system. These measures assess aspects such as user satisfaction, perceived relevance, and overall experience. For instance, users might rate how well the recommendations matched their preferences or how enjoyable the interaction was. This approach emphasizes the importance of user interpretation and personal context in evaluating the effectiveness of recommendations[1].\n",
    "\n",
    "## Objective Measures\n",
    "\n",
    "In contrast, objective measures rely on quantitative data and algorithmic performance metrics. These measures focus on specific, predefined criteria such as accuracy, precision, recall, and coverage of the recommendation lists. Objective evaluations can include metrics like the time taken to generate recommendations or the diversity of the recommended items. They aim to provide a more standardized assessment of the system's performance, independent of individual user experiences[1][2].\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "1. **Basis of Evaluation**:\n",
    "   - **Subjective**: User interpretation and feedback.\n",
    "   - **Objective**: Algorithmic performance metrics.\n",
    "\n",
    "2. **Nature of Data**:\n",
    "   - **Subjective**: Qualitative, often gathered through user surveys.\n",
    "   - **Objective**: Quantitative, based on measurable outcomes.\n",
    "\n",
    "3. **Focus**:\n",
    "   - **Subjective**: User satisfaction and perceived quality.\n",
    "   - **Objective**: System accuracy and performance metrics.\n",
    "\n",
    "4. **Evaluation Context**:\n",
    "   - **Subjective**: Often context-dependent, reflecting individual user experiences.\n",
    "   - **Objective**: More universal, aiming for consistency across evaluations.\n",
    "\n",
    "In summary, subjective measures provide insights into user satisfaction and preferences, while objective measures focus on quantifiable performance metrics of the recommender system. Both types of measures are essential for a comprehensive evaluation, as they address different aspects of user interaction and system functionality[1][2].\n",
    "\n",
    "Citations:   \n",
    "[1] https://link.springer.com/article/10.1007/s10462-022-10229-x  \n",
    "[2] https://dl.acm.org/doi/10.1145/3527449  \n",
    "[3] https://www.researchgate.net/figure/Subjective-evaluation-comparison-among-different-music-knowledge-levels-in-the-scenarios_fig2_221260748  \n",
    "[4]https://www.researchgate.net/publication/359642323_Survey_on_the_Objectives_of_Recommender_System_Measures_Solutions_Evaluation_Methodology_and_New_Perspectives  \n",
    "[5] https://www.sciencedirect.com/science/article/pii/S1110866515000341  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color:aqua;text-decoration:underline;font-weight:bold \">Question 61 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does Cosine similarity measure in a recommender system?\n",
    "\n",
    "- Similarity between user ratings\n",
    "- Correlation between user preferences\n",
    "- Euclidean distance between items\n",
    "- <span style = \"color:aqua;\">Similarity between item properties or features </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recommender system, **Cosine similarity** measures the **similarity between item properties or features**. Specifically, it assesses how similar two items are based on their feature vectors, which can be derived from user ratings or other attributes.\n",
    "\n",
    "Here's a brief explanation of each option:\n",
    "\n",
    "1. **Similarity between user ratings**: Cosine similarity is not directly used to measure the similarity between user ratings themselves; rather, it's used to measure the similarity between items or users based on their feature vectors or rating patterns.\n",
    "\n",
    "2. **Correlation between user preferences**: While cosine similarity can be used to understand similarities in user preferences indirectly (by comparing user profiles or item ratings), it is not specifically designed to measure correlation directly.\n",
    "\n",
    "3. **Euclidean distance between items**: Cosine similarity is different from Euclidean distance. Euclidean distance measures the straight-line distance between two points in space, while cosine similarity measures the angle between two vectors, focusing on orientation rather than magnitude.\n",
    "\n",
    "4. **Similarity between item properties or features**: This is the correct option. Cosine similarity calculates how similar two items are based on their feature vectors, which could include various attributes or derived features from user ratings.\n",
    "\n",
    "In summary, cosine similarity is used to measure how similar two items are based on their feature vectors, which helps in determining the relative closeness of items in a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color:olive;text-decoration:underline;font-weight:bold \">Question 62 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the primary difference between content-based filtering and collaborative-based filtering?\n",
    "\n",
    "- Content-based filtering is more computationally intensive than collaborative-based filtering\n",
    "- <span style = \"color: olive \">Content-based filtering uses item attributes for recommendations, while collaborative-based filtering uses user similarities. </span>\n",
    "- Content-based filtering is only used for books, while collaborative-based filtering is used for all items.\n",
    "- Content-based filtering uses user similarities for recommendations, while collaborative-based filtering uses item attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between content-based filtering and collaborative-based filtering is that **content-based filtering uses item attributes for recommendations, while collaborative-based filtering uses user similarities**. \n",
    "\n",
    "Content-based filtering focuses on the characteristics of items that a user has liked in the past to recommend similar items, while collaborative filtering leverages the preferences and behaviors of multiple users to suggest items that similar users have enjoyed, regardless of the specific attributes of the items themselves[1][2].\n",
    "\n",
    "Citations:  \n",
    "[1]https://thecleverprogrammer.com/2023/04/20/content-based-filtering-and-collaborative-filtering-difference/  \n",
    "[2] https://www.linkedin.com/advice/0/how-do-you-choose-between-collaborative-content-based  \n",
    "[3] https://www.upwork.com/resources/what-is-content-based-filtering  \n",
    "[4] https://www.turing.com/kb/content-based-filtering-in-recommender-systems  \n",
    "[5] https://www.ibm.com/topics/content-based-filtering  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style = \"color:magenta;text-decoration:underline;font-weight:bold \">Question 63 </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which unsupervised learning technique, in particular, faces the cold-start problem?\n",
    "\n",
    "- <span style = \"color: magenta\">A recommender system using collaborative filtering </span>\n",
    "- Hierarchical clustering\n",
    "- A recommender system using content-based filtering\n",
    "- Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsupervised learning technique that particularly faces the cold-start problem is **collaborative filtering** in recommender systems. This issue arises when a system lacks sufficient data on new users or items, making it difficult to provide personalized recommendations.\n",
    "\n",
    "### Cold-Start Problem in Collaborative Filtering\n",
    "\n",
    "1. **Definition**: The cold-start problem occurs when a recommender system cannot draw inferences about new users or items due to the absence of prior interaction data. This is especially prominent in collaborative filtering, which relies on user interactions to make recommendations. Without historical data, the system struggles to identify patterns and preferences[2].\n",
    "\n",
    "2. **Types of Cold-Start Scenarios**:\n",
    "   - **New User**: When a new user joins, there are no previous interactions to base recommendations on.\n",
    "   - **New Item**: Newly added items lack user interaction history, making them difficult to recommend.\n",
    "   - **New Community**: In a completely new system, both users and items are untested, leading to a lack of data for recommendations[2][3].\n",
    "\n",
    "3. **Comparison with Other Techniques**:\n",
    "   - **Content-Based Filtering**: This method does not face the same cold-start issues because it relies on the features of items rather than user interactions. It can recommend items based on their attributes even if no prior data exists.\n",
    "   - **Hierarchical Clustering and Principal Component Analysis**: These techniques are primarily used for data analysis and do not directly deal with user-item interactions, thus they are not typically associated with the cold-start problem in the context of recommender systems[1][3].\n",
    "\n",
    "In summary, collaborative filtering is the unsupervised learning technique most affected by the cold-start problem, as it heavily depends on user interaction data to function effectively.\n",
    "\n",
    "Citations:   \n",
    "[1] https://www.expressanalytics.com/blog/cold-start-problem/   \n",
    "[2] https://en.wikipedia.org/wiki/Cold_start_%28recommender_systems%29   \n",
    "[3] https://www.linkedin.com/pulse/cold-start-problem-recommender-systems-how-deal-them-kumar-kishalaya-jmtyc   \n",
    "[4] https://www.kdnuggets.com/2019/01/data-scientist-dilemma-cold-start-machine-learning.html   \n",
    "[5] https://stats.stackexchange.com/questions/635667/how-to-solve-the-cold-start-problem-for-recommender-systems   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Updated_Python_Dev_ALX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
