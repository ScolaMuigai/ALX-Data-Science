{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP (12 Questions, 22 Marks)\n",
    "#### Questions 42 - 53\n",
    "The practical questions of the next 12 questions should be answered using the Essay_data.csv file.\n",
    "\n",
    "This CSV file contains a personality profile, together with an essay written by an individual with that specific personality type.\n",
    "\n",
    "Once you have imported the data frame, use the dropna() function to remove rows containing missing values and reset the index.\n",
    "\n",
    "How many bi-grams can be created from the following sentence after performing the following steps in the correct order:\n",
    "\n",
    "Remove stopwords\n",
    "Remove punctuation and replace by a single white space\n",
    "Convert all text to lower case\n",
    "I’m a part-time student @explore-software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "essy_df = pd.read_csv(r'C:\\Users\\USER\\OneDrive\\CB\\ALX\\Data Science\\Integrated Exams\\Part 2\\Classification\\Data\\Essay_data.csv')\n",
    "\n",
    "essy_df.isna().sum()\n",
    "\n",
    "#Removing Null Values\n",
    "essy_df.dropna(inplace= True)\n",
    "\n",
    "#Resetting the Index\n",
    "essy_df.reset_index(drop = True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93 entries, 0 to 92\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   I/E     93 non-null     object\n",
      " 1   N/S     93 non-null     object\n",
      " 2   T/F     93 non-null     object\n",
      " 3   J/P     93 non-null     object\n",
      " 4   Essay   93 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 3.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>T/F</th>\n",
       "      <th>J/P</th>\n",
       "      <th>Essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My first 4 months at the EDSA have been filled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I joined the academy being at a crossroads of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I have been very fortunate to have the opportu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>Looking back to when one got to the academy an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>My experience at the Explore Data Science Acad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>P</td>\n",
       "      <td>A new city, new people and a completely new en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My experience at the academy has been one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>After spending a year at home while it was jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>P</td>\n",
       "      <td>The Academy has brought nothing but useful and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   I/E N/S T/F J/P                                              Essay\n",
       "0    I   S   T   J  My first 4 months at the EDSA have been filled...\n",
       "1    I   N   F   J  I joined the academy being at a crossroads of ...\n",
       "2    E   N   F   J  so far my experience has been positive and i c...\n",
       "3    I   N   F   J  I have been very fortunate to have the opportu...\n",
       "4    I   N   T   J  Looking back to when one got to the academy an...\n",
       "..  ..  ..  ..  ..                                                ...\n",
       "88   I   S   F   J  My experience at the Explore Data Science Acad...\n",
       "89   I   N   T   P  A new city, new people and a completely new en...\n",
       "90   I   N   T   J  My experience at the academy has been one of t...\n",
       "91   I   S   F   J  After spending a year at home while it was jus...\n",
       "92   E   S   F   P  The Academy has brought nothing but useful and...\n",
       "\n",
       "[93 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df.head()\n",
    "essy_df.info()\n",
    "essy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bi-Grams from I’m a part-time student @explore-software. is : 4\n"
     ]
    }
   ],
   "source": [
    "#Download Necessary NLTK data\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#Function to Remove stopwords and punctuation\n",
    "def clean_text(text):\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Remove puntuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    #Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "sentence = 'I’m a part-time student @explore-software.'\n",
    "#Clean and tokenize the sentence\n",
    "\n",
    "cleaned_sentence = clean_text(sentence)\n",
    "\n",
    "#Generate bi-grams\n",
    "bigrams = list(ngrams(cleaned_sentence, 2))\n",
    "\n",
    "\n",
    "#Count the # of bigrams\n",
    "num_bigrams = len(bigrams)\n",
    "\n",
    "print(f'Number of Bi-Grams from {sentence} is : {num_bigrams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essy_df['Cleaned_Essy'] = essy_df['Essay'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>T/F</th>\n",
       "      <th>J/P</th>\n",
       "      <th>Essay</th>\n",
       "      <th>Cleaned_Essy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My first 4 months at the EDSA have been filled...</td>\n",
       "      <td>[first, 4, months, edsa, filled, many, new, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I joined the academy being at a crossroads of ...</td>\n",
       "      <td>[joined, academy, crossroads, sorts, life, aca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>[far, experience, positive, definitely, see, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I have been very fortunate to have the opportu...</td>\n",
       "      <td>[fortunate, opportunity, join, academy, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>Looking back to when one got to the academy an...</td>\n",
       "      <td>[looking, back, one, got, academy, right, conf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  I/E N/S T/F J/P                                              Essay  \\\n",
       "0   I   S   T   J  My first 4 months at the EDSA have been filled...   \n",
       "1   I   N   F   J  I joined the academy being at a crossroads of ...   \n",
       "2   E   N   F   J  so far my experience has been positive and i c...   \n",
       "3   I   N   F   J  I have been very fortunate to have the opportu...   \n",
       "4   I   N   T   J  Looking back to when one got to the academy an...   \n",
       "\n",
       "                                        Cleaned_Essy  \n",
       "0  [first, 4, months, edsa, filled, many, new, ex...  \n",
       "1  [joined, academy, crossroads, sorts, life, aca...  \n",
       "2  [far, experience, positive, definitely, see, v...  \n",
       "3  [fortunate, opportunity, join, academy, year, ...  \n",
       "4  [looking, back, one, got, academy, right, conf...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data\n",
    "essy_df = pd.read_csv(r'C:\\Users\\USER\\OneDrive\\CB\\ALX\\Data Science\\Integrated Exams\\Part 2\\Classification\\Data\\Essay_data.csv')\n",
    "\n",
    "#Remove rows with missing values \n",
    "essy_df = essy_df.dropna().reset_index(drop= True)\n",
    "\n",
    "#Define steps to process the sentence\n",
    "def process_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and replace with a single white space\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# process the sentence\n",
    "processed_sentence = process_text(sentence)\n",
    "\n",
    "#Generate bi-grams\n",
    "bigram = list(nltk.bigrams(processed_sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i’m', 'part'),\n",
       " ('part', 'time'),\n",
       " ('time', 'student'),\n",
       " ('student', 'explore'),\n",
       " ('explore', 'software')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 'time'),\n",
       " ('time', 'student'),\n",
       " ('student', 'explore'),\n",
       " ('explore', 'software')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['part', 'time', 'student', 'explore', 'software']\n",
    "[('part', 'time'), ('time', 'student'), ('student', 'explore'), ('explore', 'software')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7368421052631579\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           N       0.74      1.00      0.85        14\n",
      "           S       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.74        19\n",
      "   macro avg       0.37      0.50      0.42        19\n",
      "weighted avg       0.54      0.74      0.63        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\Updated_Python_Dev_ALX\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\envs\\Updated_Python_Dev_ALX\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\USER\\anaconda3\\envs\\Updated_Python_Dev_ALX\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "data = essy_df\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Manually defining a list of common English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and replace it with a single whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the Essay column\n",
    "data['Essay_Cleaned'] = data['Essay'].apply(clean_text)\n",
    "\n",
    "# Step 2: Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data['Essay_Cleaned'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "y = data['N/S']  # Labels: 'N' or 'S'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.576923076923077"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df['N/S'].value_counts()\n",
    "67/26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 44\n",
    "Remove all punctuation from the essays and convert it to lower case.\n",
    "\n",
    "What is the 10th character in the first essay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaner_txt(txt):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', txt)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "essy_df['Quiz_44'] = essy_df['Essay'].apply(cleaner_txt)\n",
    "\n",
    "essy_df['Quiz_44'][0][10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df['Quiz_44'][0][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 45\n",
    "Tokenise the essays.\n",
    "\n",
    "How many tokens are in the 17th essay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaner_tokenizer_txt(txt):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', txt)\n",
    "\n",
    "    text = word_tokenize(text.lower())\n",
    "\n",
    "    return text\n",
    "\n",
    "essy_df['Quiz_45'] = essy_df['Essay'].apply(cleaner_tokenizer_txt)\n",
    "len(essy_df['Quiz_45'][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "essy_df['Quiz_45'] = essy_df['Essay'].apply(cleaner_tokenizer_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>T/F</th>\n",
       "      <th>J/P</th>\n",
       "      <th>Essay</th>\n",
       "      <th>Quiz_44</th>\n",
       "      <th>Quiz_45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My first 4 months at the EDSA have been filled...</td>\n",
       "      <td>my first 4 months at the edsa have been filled...</td>\n",
       "      <td>[my, first, 4, months, at, the, edsa, have, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I joined the academy being at a crossroads of ...</td>\n",
       "      <td>i joined the academy being at a crossroads of ...</td>\n",
       "      <td>[i, joined, the, academy, being, at, a, crossr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>[so, far, my, experience, has, been, positive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I have been very fortunate to have the opportu...</td>\n",
       "      <td>i have been very fortunate to have the opportu...</td>\n",
       "      <td>[i, have, been, very, fortunate, to, have, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>Looking back to when one got to the academy an...</td>\n",
       "      <td>looking back to when one got to the academy an...</td>\n",
       "      <td>[looking, back, to, when, one, got, to, the, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  I/E N/S T/F J/P                                              Essay  \\\n",
       "0   I   S   T   J  My first 4 months at the EDSA have been filled...   \n",
       "1   I   N   F   J  I joined the academy being at a crossroads of ...   \n",
       "2   E   N   F   J  so far my experience has been positive and i c...   \n",
       "3   I   N   F   J  I have been very fortunate to have the opportu...   \n",
       "4   I   N   T   J  Looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_44  \\\n",
       "0  my first 4 months at the edsa have been filled...   \n",
       "1  i joined the academy being at a crossroads of ...   \n",
       "2  so far my experience has been positive and i c...   \n",
       "3  i have been very fortunate to have the opportu...   \n",
       "4  looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_45  \n",
       "0  [my, first, 4, months, at, the, edsa, have, be...  \n",
       "1  [i, joined, the, academy, being, at, a, crossr...  \n",
       "2  [so, far, my, experience, has, been, positive,...  \n",
       "3  [i, have, been, very, fortunate, to, have, the...  \n",
       "4  [looking, back, to, when, one, got, to, the, a...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(essy_df['Quiz_45'][16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 46\n",
    "How does lemmatisation differ from stemming?\n",
    "\n",
    "**Lemmatisation always results in actual words, while stemming can result in non-existent words.**    \n",
    "Lemmatisation cuts off part of the word while stemming considers the morphological analysis of the words.\n",
    "Stemming always results in actual words, while lemmatisation can result in non-existent words.\n",
    "They operate the same way – they just come from different libraries (scikit-learn and NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 47 \n",
    "Use the SnowballStemmer to stem the word experiences.\n",
    "\n",
    "What is the output?\n",
    "\n",
    "‘experi’\n",
    "‘experience’\n",
    "‘experiences’\n",
    "‘experienc’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem for experiences is experi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "#Initialize \n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "word = 'experiences'\n",
    "\n",
    "stmmed_word = stemmer.stem(word)\n",
    "\n",
    "print(f'Stem for {word} is {stmmed_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Word to stem\n",
    "word = \"experiences\"\n",
    "\n",
    "# Stem the word\n",
    "stemmed_word = stemmer.stem(word)\n",
    "\n",
    "print(stemmed_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define the word to be stemmed\n",
    "word = 'experiences'\n",
    "\n",
    "# Apply stemming\n",
    "stem = stemmer.stem(word)\n",
    "\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 48\n",
    "Remove all the stop words.\n",
    "\n",
    "What is the 24th token in the 81st essay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner_tokenizer_txt_sw(txt):\n",
    "    text = re.sub(r'[^\\w\\s]', '', txt)\n",
    "\n",
    "    token = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #Remove stop words\n",
    "    texts = [word for word in token if word not in stop_words]\n",
    "\n",
    "    return texts\n",
    "\n",
    "essy_df['Quiz_48'] = essy_df['Essay'].apply(cleaner_tokenizer_txt_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experience',\n",
       " 'academy',\n",
       " 'bitter',\n",
       " 'sweet',\n",
       " 'arrived',\n",
       " 'much',\n",
       " 'excitement',\n",
       " 'eagerness',\n",
       " 'change',\n",
       " 'world',\n",
       " 'much',\n",
       " 'dismay',\n",
       " 'soon',\n",
       " 'crushed',\n",
       " 'bits',\n",
       " 'awareness',\n",
       " 'lack',\n",
       " 'knowledge',\n",
       " 'understanding',\n",
       " 'first',\n",
       " 'quite',\n",
       " 'time',\n",
       " 'affected',\n",
       " 'selfconfidence',\n",
       " 'evidently',\n",
       " 'performance',\n",
       " 'within',\n",
       " 'groups',\n",
       " 'well',\n",
       " 'function',\n",
       " 'fear',\n",
       " 'failure',\n",
       " 'moment',\n",
       " 'started',\n",
       " 'much',\n",
       " 'better',\n",
       " 'properly',\n",
       " 'understood',\n",
       " 'things',\n",
       " 'really',\n",
       " 'take',\n",
       " 'time',\n",
       " 'manifest',\n",
       " 'tried',\n",
       " 'tested',\n",
       " 'many',\n",
       " 'terrible',\n",
       " 'ideas',\n",
       " 'reach',\n",
       " 'better',\n",
       " 'level',\n",
       " 'understanding',\n",
       " 'z',\n",
       " 'thinker',\n",
       " 'fretting',\n",
       " 'obsessing',\n",
       " 'z',\n",
       " 'forgetting',\n",
       " 'b',\n",
       " 'im',\n",
       " 'developing',\n",
       " 'greater',\n",
       " 'level',\n",
       " 'self',\n",
       " 'efficacy',\n",
       " 'understand',\n",
       " 'falsely',\n",
       " 'assumed',\n",
       " 'things',\n",
       " 'would',\n",
       " 'change',\n",
       " 'night',\n",
       " 'kind',\n",
       " 'thinking',\n",
       " 'resulted',\n",
       " 'poor',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'consumed',\n",
       " 'self',\n",
       " 'connecting',\n",
       " 'team',\n",
       " 'assistance',\n",
       " 'improving',\n",
       " 'growing',\n",
       " 'falsely',\n",
       " 'assumed',\n",
       " 'able',\n",
       " 'everything',\n",
       " 'perhaps',\n",
       " 'function',\n",
       " 'comparing',\n",
       " 'high',\n",
       " 'performers',\n",
       " 'teams',\n",
       " 'intelligent',\n",
       " 'people',\n",
       " 'doubts',\n",
       " 'abilities',\n",
       " 'perhaps',\n",
       " 'reason',\n",
       " 'fear',\n",
       " 'inadequate',\n",
       " 'made',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'little',\n",
       " 'tough',\n",
       " 'especially',\n",
       " 'needed',\n",
       " 'bit',\n",
       " 'assertiveness',\n",
       " 'mostly',\n",
       " 'introverted',\n",
       " 'made',\n",
       " 'difficult',\n",
       " 'raise',\n",
       " 'issues',\n",
       " 'emotionally',\n",
       " 'uncomfortable',\n",
       " 'deal',\n",
       " 'let',\n",
       " 'slide',\n",
       " 'didnt',\n",
       " 'deal',\n",
       " 'best',\n",
       " 'manner',\n",
       " 'possible',\n",
       " 'also',\n",
       " 'fear',\n",
       " 'asking',\n",
       " 'help',\n",
       " 'overwhelmed',\n",
       " 'something',\n",
       " 'exhibited',\n",
       " 'although',\n",
       " 'us',\n",
       " 'others',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'rather',\n",
       " 'best',\n",
       " 'thing',\n",
       " 'entire',\n",
       " 'experience',\n",
       " 'able',\n",
       " 'reach',\n",
       " 'point',\n",
       " 'aware',\n",
       " 'limitations',\n",
       " 'place',\n",
       " 'especially',\n",
       " 'toxic',\n",
       " 'habits',\n",
       " 'stunt',\n",
       " 'growth',\n",
       " 'first',\n",
       " 'step',\n",
       " 'able',\n",
       " 'something',\n",
       " 'acknowledging',\n",
       " 'hardest',\n",
       " 'part',\n",
       " 'moving',\n",
       " 'selfawareness',\n",
       " 'action',\n",
       " 'least',\n",
       " 'understand',\n",
       " 'take',\n",
       " 'time',\n",
       " 'little',\n",
       " 'everyday',\n",
       " 'doesnt',\n",
       " 'stop',\n",
       " 'year',\n",
       " 'done',\n",
       " 'learning',\n",
       " 'never',\n",
       " 'stops',\n",
       " 'reason',\n",
       " 'scared',\n",
       " 'magnitude',\n",
       " 'rather',\n",
       " 'excited',\n",
       " 'learn',\n",
       " 'cant',\n",
       " 'know',\n",
       " 'everything',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'chasing',\n",
       " 'perfection',\n",
       " 'emotionally',\n",
       " 'physically',\n",
       " 'exhausting',\n",
       " 'things',\n",
       " 'continue',\n",
       " 'tough',\n",
       " 'change',\n",
       " 'iskeeps',\n",
       " 'occurring',\n",
       " 'change',\n",
       " 'always',\n",
       " 'tough',\n",
       " 'deal',\n",
       " 'however',\n",
       " 'able',\n",
       " 'go',\n",
       " 'others',\n",
       " 'amazing',\n",
       " 'experience',\n",
       " 'one',\n",
       " 'would',\n",
       " 'trade',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'valuable',\n",
       " 'thing',\n",
       " 'learnt',\n",
       " 'emotions',\n",
       " 'cloud',\n",
       " 'judgement',\n",
       " 'affects',\n",
       " 'performance',\n",
       " 'use',\n",
       " 'logic',\n",
       " 'height',\n",
       " 'stress',\n",
       " 'helps',\n",
       " 'us',\n",
       " 'stop',\n",
       " 'emotions',\n",
       " 'clouding',\n",
       " 'judgement',\n",
       " 'making',\n",
       " 'us',\n",
       " 'make',\n",
       " 'irrational',\n",
       " 'decisions',\n",
       " 'real',\n",
       " 'value',\n",
       " 'education',\n",
       " 'wont',\n",
       " 'seen',\n",
       " 'results',\n",
       " 'sprints',\n",
       " 'belt',\n",
       " 'scores',\n",
       " 'impact',\n",
       " 'make',\n",
       " 'lives',\n",
       " 'ultimately',\n",
       " 'wherever',\n",
       " 'find',\n",
       " 'beyond',\n",
       " 'academy',\n",
       " 'keep',\n",
       " 'learning',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'science',\n",
       " 'importantly',\n",
       " 'learning',\n",
       " 'world',\n",
       " 'around',\n",
       " 'us']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df['Quiz_48'][80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'selfconfidence'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df['Quiz_48'][80][23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 49\n",
    "How many unique words are in these essays (after we have removed the stopwords)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Words: 3406\n"
     ]
    }
   ],
   "source": [
    "essy_words = [word for word in essy_df['Quiz_48']]\n",
    "\n",
    "#Combining all essys into a single essy\n",
    "all_words = []\n",
    "for essy in  essy_df['Quiz_48']:\n",
    "    all_words.extend(essy)\n",
    "\n",
    "unique_words = np.unique(all_words)\n",
    "count_unique_words = len(unique_words)\n",
    "\n",
    "print(f'Count of Unique Words: {count_unique_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 50\n",
    "Create a bag of words and use it to determine how many times ‘time’ was mentioned in the 56th essay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'time' appears 2 times in hte 56th Essy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "bow = Counter(essy_df['Quiz_48'].iloc[55])\n",
    "\n",
    "#Counting occurence of 'time'\n",
    "\n",
    "time_count = bow['time']\n",
    "\n",
    "print(f\"The word 'time' appears {time_count} times in hte 56th Essy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>T/F</th>\n",
       "      <th>J/P</th>\n",
       "      <th>Essay</th>\n",
       "      <th>Quiz_44</th>\n",
       "      <th>Quiz_45</th>\n",
       "      <th>Quiz_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My first 4 months at the EDSA have been filled...</td>\n",
       "      <td>my first 4 months at the edsa have been filled...</td>\n",
       "      <td>[my, first, 4, months, at, the, edsa, have, be...</td>\n",
       "      <td>[first, 4, months, edsa, filled, many, new, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I joined the academy being at a crossroads of ...</td>\n",
       "      <td>i joined the academy being at a crossroads of ...</td>\n",
       "      <td>[i, joined, the, academy, being, at, a, crossr...</td>\n",
       "      <td>[joined, academy, crossroads, sorts, life, aca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>[so, far, my, experience, has, been, positive,...</td>\n",
       "      <td>[far, experience, positive, definitely, see, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I have been very fortunate to have the opportu...</td>\n",
       "      <td>i have been very fortunate to have the opportu...</td>\n",
       "      <td>[i, have, been, very, fortunate, to, have, the...</td>\n",
       "      <td>[fortunate, opportunity, join, academy, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>Looking back to when one got to the academy an...</td>\n",
       "      <td>looking back to when one got to the academy an...</td>\n",
       "      <td>[looking, back, to, when, one, got, to, the, a...</td>\n",
       "      <td>[looking, back, one, got, academy, right, conf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  I/E N/S T/F J/P                                              Essay  \\\n",
       "0   I   S   T   J  My first 4 months at the EDSA have been filled...   \n",
       "1   I   N   F   J  I joined the academy being at a crossroads of ...   \n",
       "2   E   N   F   J  so far my experience has been positive and i c...   \n",
       "3   I   N   F   J  I have been very fortunate to have the opportu...   \n",
       "4   I   N   T   J  Looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_44  \\\n",
       "0  my first 4 months at the edsa have been filled...   \n",
       "1  i joined the academy being at a crossroads of ...   \n",
       "2  so far my experience has been positive and i c...   \n",
       "3  i have been very fortunate to have the opportu...   \n",
       "4  looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_45  \\\n",
       "0  [my, first, 4, months, at, the, edsa, have, be...   \n",
       "1  [i, joined, the, academy, being, at, a, crossr...   \n",
       "2  [so, far, my, experience, has, been, positive,...   \n",
       "3  [i, have, been, very, fortunate, to, have, the...   \n",
       "4  [looking, back, to, when, one, got, to, the, a...   \n",
       "\n",
       "                                             Quiz_48  \n",
       "0  [first, 4, months, edsa, filled, many, new, ex...  \n",
       "1  [joined, academy, crossroads, sorts, life, aca...  \n",
       "2  [far, experience, positive, definitely, see, v...  \n",
       "3  [fortunate, opportunity, join, academy, year, ...  \n",
       "4  [looking, back, one, got, academy, right, conf...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9036\\852787224.py:1: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  pd.value_counts(essy_df['Essay'].str.lower().iloc[55].split()).to_dict()['time']\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9036\\852787224.py:1: FutureWarning: value_counts with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  pd.value_counts(essy_df['Essay'].str.lower().iloc[55].split()).to_dict()['time']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(essy_df['Essay'].str.lower().iloc[55].split()).to_dict()['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(essy_df['Essay'].str.lower().iloc[55].split())['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 51\n",
    "Words that appear at least twice account for what percentage of the total number of words in the essays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_all_words = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of words that appeat atleast twice: 48%\n"
     ]
    }
   ],
   "source": [
    "words_atleast_twice = [key for key,val in counter_all_words.items() if counter_all_words[key] >= 2]\n",
    "count_words_atleast_twice = len(words_atleast_twice)\n",
    "\n",
    "p = count_words_atleast_twice/count_unique_words\n",
    "\n",
    "print(f'Proportion of words that appeat atleast twice: {p:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 52\n",
    "What is the most commonly mentioned word by ENFJ personalities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Word in ENFJ Personality is: \u001b[1;4;92mteam\u001b[0m Appearing \u001b[1;4;92m33\u001b[0m times\n"
     ]
    }
   ],
   "source": [
    "#Creating Personality Col\n",
    "essy_df['Personality'] = essy_df['I/E'] + essy_df['N/S'] + essy_df['T/F'] + essy_df['J/P']\n",
    "#Filter for ENFJ Personalities\n",
    "ENFJ_df = essy_df[essy_df['Personality'] == 'ENFJ']\n",
    "\n",
    "#Combining th essy into one essy\n",
    "ENFJ_essys = []\n",
    "for essy in ENFJ_df['Quiz_48']:\n",
    "    ENFJ_essys.extend(essy)\n",
    "\n",
    "ENFJ_counter = pd.Series(ENFJ_essys).value_counts().sort_values(ascending= False)\n",
    "\n",
    "most_com_word = ENFJ_counter.index[0]\n",
    "n = ENFJ_counter.max()\n",
    "\n",
    "print(f'Most Common Word in ENFJ Personality is: \\033[1;4;92m{most_com_word}\\033[0m Appearing \\033[1;4;92m{n}\\33[0m times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 53\n",
    "Create a new column in the data frame containing the bi-grams from each essay.\n",
    "\n",
    "What is the 109th bi-gram in the 70th essay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I/E</th>\n",
       "      <th>N/S</th>\n",
       "      <th>T/F</th>\n",
       "      <th>J/P</th>\n",
       "      <th>Essay</th>\n",
       "      <th>Quiz_44</th>\n",
       "      <th>Quiz_45</th>\n",
       "      <th>Quiz_48</th>\n",
       "      <th>Personality</th>\n",
       "      <th>bi-grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>My first 4 months at the EDSA have been filled...</td>\n",
       "      <td>my first 4 months at the edsa have been filled...</td>\n",
       "      <td>[my, first, 4, months, at, the, edsa, have, be...</td>\n",
       "      <td>[first, 4, months, edsa, filled, many, new, ex...</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>[(first, 4), (4, months), (months, edsa), (eds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I joined the academy being at a crossroads of ...</td>\n",
       "      <td>i joined the academy being at a crossroads of ...</td>\n",
       "      <td>[i, joined, the, academy, being, at, a, crossr...</td>\n",
       "      <td>[joined, academy, crossroads, sorts, life, aca...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>[(joined, academy), (academy, crossroads), (cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>so far my experience has been positive and i c...</td>\n",
       "      <td>[so, far, my, experience, has, been, positive,...</td>\n",
       "      <td>[far, experience, positive, definitely, see, v...</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>[(far, experience), (experience, positive), (p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>I have been very fortunate to have the opportu...</td>\n",
       "      <td>i have been very fortunate to have the opportu...</td>\n",
       "      <td>[i, have, been, very, fortunate, to, have, the...</td>\n",
       "      <td>[fortunate, opportunity, join, academy, year, ...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>[(fortunate, opportunity), (opportunity, join)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>Looking back to when one got to the academy an...</td>\n",
       "      <td>looking back to when one got to the academy an...</td>\n",
       "      <td>[looking, back, to, when, one, got, to, the, a...</td>\n",
       "      <td>[looking, back, one, got, academy, right, conf...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>[(looking, back), (back, one), (one, got), (go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  I/E N/S T/F J/P                                              Essay  \\\n",
       "0   I   S   T   J  My first 4 months at the EDSA have been filled...   \n",
       "1   I   N   F   J  I joined the academy being at a crossroads of ...   \n",
       "2   E   N   F   J  so far my experience has been positive and i c...   \n",
       "3   I   N   F   J  I have been very fortunate to have the opportu...   \n",
       "4   I   N   T   J  Looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_44  \\\n",
       "0  my first 4 months at the edsa have been filled...   \n",
       "1  i joined the academy being at a crossroads of ...   \n",
       "2  so far my experience has been positive and i c...   \n",
       "3  i have been very fortunate to have the opportu...   \n",
       "4  looking back to when one got to the academy an...   \n",
       "\n",
       "                                             Quiz_45  \\\n",
       "0  [my, first, 4, months, at, the, edsa, have, be...   \n",
       "1  [i, joined, the, academy, being, at, a, crossr...   \n",
       "2  [so, far, my, experience, has, been, positive,...   \n",
       "3  [i, have, been, very, fortunate, to, have, the...   \n",
       "4  [looking, back, to, when, one, got, to, the, a...   \n",
       "\n",
       "                                             Quiz_48 Personality  \\\n",
       "0  [first, 4, months, edsa, filled, many, new, ex...        ISTJ   \n",
       "1  [joined, academy, crossroads, sorts, life, aca...        INFJ   \n",
       "2  [far, experience, positive, definitely, see, v...        ENFJ   \n",
       "3  [fortunate, opportunity, join, academy, year, ...        INFJ   \n",
       "4  [looking, back, one, got, academy, right, conf...        INTJ   \n",
       "\n",
       "                                            bi-grams  \n",
       "0  [(first, 4), (4, months), (months, edsa), (eds...  \n",
       "1  [(joined, academy), (academy, crossroads), (cr...  \n",
       "2  [(far, experience), (experience, positive), (p...  \n",
       "3  [(fortunate, opportunity), (opportunity, join)...  \n",
       "4  [(looking, back), (back, one), (one, got), (go...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "def gen_bigrams(x):\n",
    "    return [pair for pair in bigrams(x)]\n",
    "essy_df['bi-grams'] = essy_df['Quiz_48'].apply(gen_bigrams)\n",
    "\n",
    "essy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 109th bi grams in the 70th Essy is:  \u001b[1;92m('quite', 'well')\n"
     ]
    }
   ],
   "source": [
    "_70_essy = essy_df['bi-grams'][69]\n",
    "\n",
    "_109_bigrams = _70_essy[108]\n",
    "\n",
    "print(f'The 109th bi grams in the 70th Essy is:  \\033[1;92m{_109_bigrams}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Updated_Python_Dev_ALX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
